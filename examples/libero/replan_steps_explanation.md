# replan_steps、action_chunk 和模拟器频率的关系详解

## 核心概念

### 1. **模拟器频率 (Simulator Frequency)**
- **Libero环境的默认频率**: **20 Hz** (每秒20步)
- 这意味着：每步的时间间隔 = 1/20 = **0.05秒** (50毫秒)
- 模拟器会以固定频率运行，每0.05秒执行一个动作

### 2. **action_chunk (Policy输出的动作序列)**
- **长度**: 通常是 **50个动作** (这是policy模型一次推理输出的长度)
- **含义**: Policy基于当前观察，预测未来50步应该执行的动作序列
- **注意**: 这50个动作是"计划"，不一定全部执行

### 3. **replan_steps (重新规划步数)**
- **默认值**: **5**
- **含义**: 从action_chunk中取前N个动作执行，执行完这N个动作后，重新调用policy推理
- **作用**: 控制policy推理的频率

## 具体执行流程示例

假设：
- `replan_steps = 5`
- `action_chunk` 长度 = 50
- 模拟器频率 = 20 Hz

### 时间线示例

```
时间轴 (秒)    模拟器步数    执行的动作来源        说明
─────────────────────────────────────────────────────────────
0.00s          Step 0       action_chunk[0]        ← 第一次推理，获取50个动作
0.05s          Step 1       action_chunk[1]        执行前5个动作
0.10s          Step 2       action_chunk[2]        (replan_steps=5)
0.15s          Step 3       action_chunk[3]        
0.20s          Step 4       action_chunk[4]        
─────────────────────────────────────────────────────────────
0.25s          Step 5       action_chunk[0]        ← 第二次推理，重新获取50个动作
0.30s          Step 6       action_chunk[1]        执行新的前5个动作
0.35s          Step 7       action_chunk[2]        
0.40s          Step 8       action_chunk[3]        
0.45s          Step 9       action_chunk[4]        
─────────────────────────────────────────────────────────────
0.50s          Step 10      action_chunk[0]        ← 第三次推理...
...
```

## 关键代码逻辑

```python
# 1. 初始化：action_plan 是一个队列，存储待执行的动作
action_plan = collections.deque()

# 2. 主循环
while t < max_steps:
    # 如果 action_plan 空了，需要重新推理
    if not action_plan:
        # 调用policy推理，获取50个动作
        response = client.infer(element)
        action_chunk = response["actions"]  # 长度=50
        
        # 只取前 replan_steps 个动作加入执行队列
        action_plan.extend(action_chunk[: args.replan_steps])  # 取前5个
    
    # 从队列中取出一个动作执行
    action = action_plan.popleft()
    
    # 在模拟器中执行这个动作（耗时0.05秒）
    obs, reward, done, info = env.step(action.tolist())
    t += 1
```

## 频率计算

### Policy推理频率 (Replanning Frequency)

```
推理频率 = 模拟器频率 / replan_steps
        = 20 Hz / 5
        = 4 Hz
```

**含义**: 每0.25秒（5步）重新推理一次

### 控制频率 (Control Frequency)

```
控制频率 = 模拟器频率
        = 20 Hz
```

**含义**: 每0.05秒执行一个动作（这是模拟器的固有频率）

## 不同 replan_steps 的对比

### 情况1: replan_steps = 1 (最高频率推理)

```
推理频率 = 20 Hz / 1 = 20 Hz
每次推理间隔 = 0.05秒
```

- **优点**: 响应最快，能立即适应环境变化
- **缺点**: 推理开销大，如果推理延迟 > 0.05秒，会阻塞模拟器

### 情况2: replan_steps = 5 (默认)

```
推理频率 = 20 Hz / 5 = 4 Hz
每次推理间隔 = 0.25秒
```

- **优点**: 平衡了响应速度和推理开销
- **缺点**: 如果环境变化快，可能反应不够及时

### 情况3: replan_steps = 10 (低频推理)

```
推理频率 = 20 Hz / 10 = 2 Hz
每次推理间隔 = 0.5秒
```

- **优点**: 推理开销小
- **缺点**: 响应慢，可能错过快速变化

## 实际案例：推理延迟的影响

假设：
- 模拟器频率 = 20 Hz (每步0.05秒)
- Policy推理延迟 = 0.1秒 (100毫秒)
- `replan_steps = 5`

### 执行时间线

```
时间    操作                    耗时
─────────────────────────────────────
0.00s   开始推理                 0.10s (阻塞)
0.10s   推理完成，获取50个动作
0.10s   执行 action_chunk[0]    0.05s
0.15s   执行 action_chunk[1]    0.05s
0.20s   执行 action_chunk[2]    0.05s
0.25s   执行 action_chunk[3]    0.05s
0.30s   执行 action_chunk[4]    0.05s
0.35s   开始下一次推理           0.10s (阻塞)
0.45s   推理完成...
```

**问题**: 如果推理延迟(0.1s) > 单步时间(0.05s)，会导致模拟器等待

### 解决方案

代码中已经添加了推理延迟监控和建议：

```python
# 如果平均推理延迟 = 0.1秒
mean_latency = 0.1

# 建议的 replan_steps
suggested_replan_steps = ceil(20 * 0.1) = ceil(2.0) = 2

# 这样推理频率 = 20 / 2 = 10 Hz
# 每次推理间隔 = 0.1秒，刚好匹配推理延迟
```

## action_chunk 长度为什么是50？

- **历史原因**: 训练时通常使用较长的动作序列
- **实际使用**: 我们只取前 `replan_steps` 个，剩余的47个动作被丢弃
- **为什么不用完**: 
  - 环境会变化，旧的预测可能不准确
  - 频繁重新规划能更好地适应动态环境

## 总结

| 参数 | 值 | 含义 |
|------|-----|------|
| 模拟器频率 | 20 Hz | 每秒执行20个动作 |
| action_chunk长度 | 50 | Policy一次推理输出的动作数 |
| replan_steps | 5 | 每次推理后执行的动作数 |
| 推理频率 | 4 Hz | 每0.25秒推理一次 |
| 控制频率 | 20 Hz | 每0.05秒执行一个动作 |

**关键关系**:
```
推理频率 = 模拟器频率 / replan_steps
推理间隔 = replan_steps / 模拟器频率 = replan_steps * 0.05秒
```

**建议**: 
- 如果推理延迟 < 0.05秒，可以用 `replan_steps = 1` (最高响应)
- 如果推理延迟 ≈ 0.1秒，建议用 `replan_steps = 2-5` (平衡)
- 如果推理延迟 > 0.2秒，建议用 `replan_steps >= 5` (避免阻塞)

## 为什么 replan_steps=5 可能比 replan_steps=1 效果更好？

虽然理论上 `replan_steps=1` 响应最快，但实际实验中发现 `replan_steps=5` 效果更好。主要原因：

### 1. **动作平滑性 (Action Smoothness)**

**问题**: 如果每步都重新规划，policy可能会对微小的观察噪声过度反应

**例子**:
```
replan_steps = 1:
Step 0: 观察 → 推理 → action[0] = [0.05, 0.02, 0.01]  (向右移动)
Step 1: 观察(可能有噪声) → 推理 → action[0] = [-0.03, 0.02, 0.01]  (向左移动)
Step 2: 观察 → 推理 → action[0] = [0.04, 0.02, 0.01]  (又向右)
结果: 轨迹抖动，机器人左右摇摆

replan_steps = 5:
Step 0: 观察 → 推理 → 获取5个动作 [a0, a1, a2, a3, a4]
Step 1-4: 执行 a1, a2, a3, a4 (不重新推理)
结果: 轨迹平滑，动作连贯
```

### 2. **Policy训练假设 (Training Assumptions)**

**关键点**: Policy在训练时，通常假设动作序列会被**连续执行**

- **训练时**: Policy看到观察，预测50个动作，这些动作会被连续执行
- **推理时 replan_steps=1**: 只执行第一个动作就重新规划，不符合训练假设
- **推理时 replan_steps=5**: 执行前5个动作，更接近训练时的假设

### 3. **观察噪声放大 (Observation Noise Amplification)**

**问题**: 每一步的观察可能有微小噪声（图像噪声、传感器噪声等）

```
replan_steps = 1:
- 每步都基于新观察重新规划
- 观察噪声会被放大，导致动作不稳定
- 就像"过度校正"：看到一点偏差就立即修正，反而造成抖动

replan_steps = 5:
- 每5步才重新规划
- 观察噪声的影响被平均化
- 动作更稳定，不会对微小偏差过度反应
```

### 4. **动作序列的连贯性 (Action Sequence Coherence)**

**Policy输出的50个动作是一个连贯的序列**:
- `action_chunk[0]` 到 `action_chunk[4]` 是policy基于**同一个观察**预测的
- 这些动作是**相互协调**的，形成一个连贯的轨迹段
- 如果只执行第一个就重新规划，会破坏这种连贯性

**例子**:
```
Policy预测的5个动作:
[0] = [0.05, 0.0, 0.0]    # 向右移动
[1] = [0.05, 0.0, 0.0]    # 继续向右
[2] = [0.03, 0.02, 0.0]   # 向右并稍微向前
[3] = [0.02, 0.03, 0.0]   # 逐渐转向目标
[4] = [0.01, 0.04, 0.0]   # 接近目标

如果 replan_steps=1:
- 只执行 [0]，然后重新规划
- 新的规划可能完全不同，破坏了连贯性

如果 replan_steps=5:
- 执行完整的 [0-4] 序列
- 保持了policy预测的连贯轨迹
```

### 5. **计算开销 vs 性能权衡**

虽然推理延迟可能很小，但还有其他开销：

```
replan_steps = 1:
- 推理频率: 20 Hz
- 总推理次数: 200次 (10秒任务)
- 每次推理都有网络延迟、序列化开销等

replan_steps = 5:
- 推理频率: 4 Hz  
- 总推理次数: 40次 (10秒任务)
- 减少了80%的推理开销
- 但性能反而更好（因为动作更平滑）
```

### 6. **实际延迟分析**

**注意**: 虽然单次推理延迟可能很小（比如0.01秒），但：

1. **累积延迟**: 如果 `replan_steps=1`，每0.05秒推理一次，但推理需要0.01秒，实际控制频率会降低
2. **网络抖动**: 网络延迟可能有波动，`replan_steps=1` 更容易受到抖动影响
3. **系统负载**: 高频推理会增加系统负载，可能影响其他部分

## 最佳实践建议

基于以上分析：

| replan_steps | 适用场景 | 优缺点 |
|-------------|---------|--------|
| **1** | 需要极快响应，环境变化非常快 | ✅ 响应最快<br>❌ 可能抖动，开销大 |
| **2-5** | **推荐**：大多数场景 | ✅ 平衡响应和稳定性<br>✅ 动作平滑<br>✅ 符合训练假设 |
| **5-10** | 环境变化慢，需要稳定轨迹 | ✅ 非常平滑<br>❌ 响应较慢 |

**实验建议**:
- 从 `replan_steps=5` 开始（这是默认值，通常效果最好）
- 如果任务需要极快响应，可以尝试 `replan_steps=2-3`
- 如果任务需要非常平滑的轨迹，可以尝试 `replan_steps=10`
- 观察代码中的 `avg_correction` 指标：如果很大，说明频繁重新规划导致轨迹不稳定
